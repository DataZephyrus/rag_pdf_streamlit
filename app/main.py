# rag_pdf_streamlit/app/main.py
import streamlit as st
import os

# Print directory structure for debugging
print("Current directory:", os.getcwd())
print("Files in current directory:", os.listdir())
if os.path.exists("app"):
    print("Files in app directory:", os.listdir("app"))

# Set page configuration
st.set_page_config(
    page_title="PDF Chat Assistant",
    page_icon="📄",
    layout="wide",
)

# Main application header
st.title("📄 PDF Chat Assistant")
st.markdown("### Upload your PDF and chat with its contents")

# Sidebar for document upload
with st.sidebar:
    st.header("Document Upload")
    uploaded_file = st.file_uploader("Choose a PDF file", type="pdf")
    
    if uploaded_file is not None:
        st.success(f"Uploaded: {uploaded_file.name}")
        # Button to process the document
        if st.button("Process Document"):
            st.info("Document processing would happen here...")
    
    st.markdown("---")
    st.subheader("Options")
    temperature = st.slider("AI Temperature", min_value=0.0, max_value=1.0, value=0.7, step=0.1,
                          help="Lower values make responses more deterministic, higher values more creative")
    
    model = st.selectbox("Select Model", ["gpt-3.5-turbo", "gpt-4"], 
                        help="Choose the model for generating responses")

# Main chat interface
if uploaded_file is None:
    # No file uploaded yet - show welcome message
    st.info("👈 Please upload a PDF document from the sidebar to get started.")
    
    # Feature sections using columns instead of cards
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("### 📚 Document Analysis")
        st.markdown("Upload PDFs and extract meaningful insights through conversation.")
    with col2:
        st.markdown("### 🔍 Contextual Search")
        st.markdown("Ask questions about specific information in your documents.")
    with col3:
        st.markdown("### 💡 Smart Summaries")
        st.markdown("Get concise summaries of lengthy documents with a simple prompt.")
else:
    # File uploaded - show chat interface
    st.subheader("Chat with your document")
    
    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": f"I'm ready to answer questions about {uploaded_file.name}. What would you like to know?"}
        ]
    
    # Display chat messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Chat input
    if prompt := st.chat_input("Ask a question about your document"):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Display user message
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Generate and display assistant response
        with st.chat_message("assistant"):
            response = f"This is a simulated response to your query: '{prompt}'. In a full implementation, this would be generated by an AI model based on the content of your PDF."
            st.markdown(response)
        
        # Add assistant response to chat history
        st.session_state.messages.append({"role": "assistant", "content": response})

# Footer
st.markdown("---")
st.caption("PDF Chat Assistant • Built with Streamlit & LangChain")